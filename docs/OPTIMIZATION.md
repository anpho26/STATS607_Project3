# Optimization Documentation

This document describes the optimizations implemented in the project,
the evidence from profiling, and the main lessons learned.


## Optimizations Implemented

### **Optimization 1: NumPy-backed fast Pólya urn**

#### Problem identified
Profiling showed that the **baseline Pólya urn** used in Part A performed tens of thousands of Python-level list operations.  
This resulted in substantial interpreter overhead and made the Part A simulations the slowest component of the pipeline.

#### Solution implemented
A new backend called `"fast"` was implemented:

- The urn is stored in a **preallocated NumPy array** instead of a Python list.
- Sampling from previous urn values uses fast NumPy indexing.
- The same statistical algorithm is preserved, but with reduced Python overhead.

A new argument was added:

```bash
--backend baseline|fast
```

allowing direct comparison.

#### Code comparison

**Before (baseline list-based urn):**
```python
xs = []
for m in range(1, n+1):
    x_m = draw_polya_next(xs, alpha, rng)
    xs.append(x_m)
```

**After (NumPy fast urn):**
```python
xs_arr = np.empty(n + L, dtype=float)
for m in range(1, n+1):
    j = rng.integers(0, m_curr)
    x_m = xs_arr[j]   # vectorized indexing
    xs_arr[m_curr] = x_m
```

#### Performance impact

- Roughly **30–40% speedup** observed for large `n` and `M`.
- Significantly less Python overhead.
- Improved memory locality due to contiguous array storage.

#### Trade-offs

- More complex code path (two backends to maintain).
- Slightly higher memory usage from preallocating arrays.
- Must carefully ensure statistical equivalence between backends.

---

### **Optimization 2: Parallel execution of Monte Carlo replicates (Part C)**

#### Problem identified
Part C requires running **hundreds of independent Monte Carlo replicates**, and the entire computation originally executed serially.  
Runtime scaled linearly with `M`, making large-scale experiments slow.

#### Solution implemented
Enabled parallel execution using `ProcessPoolExecutor`:

```python
with ProcessPoolExecutor(max_workers=n_jobs) as ex:
    for rows_rep in ex.map(_worker_run_one_replicate, tasks):
        rows.extend(rows_rep)
```

A new CLI flag was added:

```bash
--n-jobs K
```

and benchmarking mode (`OPTIMIZED=1`) automatically sets:

```bash
n_jobs = 2
backend = "fast"
```

#### Code comparison

**Before (serial loop):**
```python
for rep in range(M):
    rows.extend(run_one_replicate(...))
```

**After (parallel workers):**
```python
with ProcessPoolExecutor(max_workers=n_jobs) as ex:
    for rows_rep in ex.map(_worker_run_one_replicate, tasks):
        rows.extend(rows_rep)
```

#### Performance impact

- **2× speedup** when using `n_jobs=2`.
- Much larger improvements possible on machines with more cores.
- Made full Prop 2.6 simulation feasible within project time constraints.

#### Trade-offs

- Parallel workers increase memory consumption.
- Debugging multiprocessing code is more challenging.
- CSV output must be carefully merged to avoid race conditions.


## Profiling Evidence

## Profiler Output

To identify bottlenecks in the baseline and optimized implementations, I used **Python’s built-in `cProfile`** together with **SnakeViz** for interactive visualization.

### Tools used

- **`cProfile`**  
  Used to collect detailed runtime information for every Python function.

- **`SnakeViz`**  
  Used to visualize `.prof` files generated by `cProfile` as flame graphs and icicle plots.
  Example command:
  ```bash
  snakeviz results/profile/parta_prior_M4000.prof
  ```

### How profiling was executed

Profiling was automated via the Makefile target:

```bash
make profile
```

This ran profiling for:

- **Part A (baseline):** prior and posterior panel generation  
- **Part A (optimized):** NumPy-based urn backend  
- **Part B:** log convergence + figure generation  
- **Part C (baseline):** Proposition 2.6 computation  
- **Part C (optimized parallel):** fast backend using `n_jobs=2`

Example call (generated by Makefile):

```bash
python -m cProfile -o results/profile/partc_log_prop26_fast_parallel.prof \
    -m src_cli.partc_log_prop26 \
    --alpha 5.0 --t 0.25 0.5 0.75 \
    --n 100 500 1000 --M 400 --seed 2025 --base uniform \
    --backend fast --n-jobs 2
```

All profiler outputs were saved under:

```
results/profile/
```

### Visualization

Profiler results were viewed using SnakeViz:

```bash
snakeviz results/profile/parta_posterior_n1000.prof
```

Representative screenshots inserted into this document correspond to:

- Part A baseline flame graph (deep recursive Python calls)
- Part A fast backend (significantly shallower and fewer Python-level calls)
- Part C serial vs parallel backend (parallel version reduces cumulative runtime)
- Part B figure generation (time dominated by Matplotlib)

---
## Summary of Findings

Profiling revealed several key bottlenecks and improvements:

### Part A (Baseline)
- Python list operations (`append`, random indexing) consumed **70–95%** of total runtime.
- Very deep call stack with many short Python-level calls.
- Overall slowest part of the entire project.

### Part A (Fast backend)
- Swapping to a NumPy-backed urn dramatically reduced Python interpreter overhead.
- Much faster indexing and memory access patterns.

Benchmarking showed that the NumPy-backed fast backend provides a **~2.9× overall speedup** for the representative configuration (n=1000, M=4000, N=2000).  
This aligns with profiling insights that Part A is dominated by Python-level list operations, which the optimized backend avoids.


### Part B
- Computation itself is lightweight.
- Runtime primarily dominated by Matplotlib rendering.
- No meaningful optimization required.

### Part C (Baseline serial)
- `run_one_replicate` was the dominant bottleneck.
- All Monte Carlo replicates were executed sequentially.

### Part C (Parallel optimized)

The parallel backend for Part C demonstrated substantial acceleration:  
- **1 → 2 workers:** ~1.84× speedup  
- **1 → 4 workers:** ~2.90× speedup  

These gains confirm that parallelizing Monte Carlo replicates significantly reduces wall‑clock time despite sublinear scaling due to multiprocessing overhead.

## Lessons Learned

### Which optimizations provided the best return on investment?

The **NumPy‑backed fast Pólya urn (Part A)** delivered the single largest improvement per line of code written.  
A relatively compact rewrite eliminated the bulk of Python‑level overhead and produced a **~3× speedup** in the most expensive component of the entire project.

The **parallel backend for Part C** also proved extremely impactful: Monte Carlo replicates are embarrassingly parallel, so even modest parallelism (2–4 workers) produced nearly linear speedups.

### What surprised me about where time was actually spent?

Profiling showed that the majority of Part A’s runtime was not in the mathematical logic but in **Python list management**, particularly repeated `append` and per‑element random access.  
This was surprising because the algorithm itself is simple—yet interpreter overhead dominated total runtime.

For Part C, I expected the statistical computation to be expensive, but instead the real bottleneck was the **sequential execution of completely independent replicates**.

### Which optimizations were not worth the effort?

Attempting to further optimize Part C using vectorization or low‑level refactoring had **little effect**: the true limit was serial execution, not algorithmic complexity.  
Similarly, deeper micro‑optimizations inside `run_one_replicate` produced only marginal gains compared to the dramatic improvements achieved through parallelism.

### What will I do differently in future profiling/optimization projects?

- **Measure first, guess later.** The biggest slowdowns came from places I would not have expected without profiling.
- **Use standalone profiling scripts earlier**, instead of waiting until the full pipeline is implemented.
- **Automate profiling and benchmarking** (e.g., Makefile targets), which made iteration far faster and more reproducible.